{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T21:55:04.123899Z",
     "start_time": "2024-05-31T21:55:01.176019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, Input, Activation, BatchNormalization, Add, UpSampling2D, ZeroPadding2D\n",
    "from keras.utils import get_file\n",
    "import numpy as np\n",
    "\n",
    "def normalize_image(image):\n",
    "    \"\"\"Normalize the image for the Hourglass network.\n",
    "    # Arguments\n",
    "      image: BGR uint8\n",
    "    # Returns\n",
    "      float32 image with the same shape as the input\n",
    "    \"\"\"\n",
    "    mean = [0.40789655, 0.44719303, 0.47026116]\n",
    "    std = [0.2886383, 0.27408165, 0.27809834]\n",
    "    return ((np.float32(image) / 255.) - mean) / std\n",
    "\n",
    "\n",
    "def HourglassNetwork(heads, num_stacks, cnv_dim=256, dims=[256, 384, 384, 384, 512]):\n",
    "    \"\"\"Instantiates the Hourglass architecture.\n",
    "    Optionally loads weights pre-trained on COCO.\n",
    "    Note that the data format convention used by the model is\n",
    "    the one specified in your Keras config at `~/.keras/keras.json`.\n",
    "    # Arguments\n",
    "        num_stacks: number of hourglass modules.\n",
    "        cnv_dim: number of filters after the resolution is decreased.\n",
    "        inres: network input shape, should be a multiple of 128.\n",
    "        weights: one of `None` (random initialization),\n",
    "              'ctdet_coco' (pre-training on COCO for 2D object detection),\n",
    "              'hpdet_coco' (pre-training on COCO for human pose detection),\n",
    "              or the path to the weights file to be loaded.\n",
    "        dims: numbers of channels in the hourglass blocks.\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "    # Raises\n",
    "        ValueError: in case of invalid argument for `weights`,\n",
    "            or invalid input shape.\n",
    "    \"\"\"\n",
    "    input_layer = Input(shape=(512, 512, 1), name='HGInput')\n",
    "    inter = pre(input_layer, cnv_dim)\n",
    "    prev_inter = None\n",
    "    outputs = []\n",
    "    for i in range(num_stacks):\n",
    "        prev_inter = inter\n",
    "        _heads, inter = hourglass_module(heads, inter, cnv_dim, i, dims)\n",
    "        outputs.extend(_heads)\n",
    "        if i < num_stacks - 1:\n",
    "            inter_ = Conv2D(cnv_dim, 1, use_bias=False, name='inter_.%d.0' % i)(prev_inter)\n",
    "            inter_ = BatchNormalization(epsilon=1e-5, name='inter_.%d.1' % i)(inter_)\n",
    "    \n",
    "            cnv_ = Conv2D(cnv_dim, 1, use_bias=False, name='cnv_.%d.0' % i)(inter)\n",
    "            cnv_ = BatchNormalization(epsilon=1e-5, name='cnv_.%d.1' % i)(cnv_)\n",
    "    \n",
    "            inter = Add(name='inters.%d.inters.add' % i)([inter_, cnv_])\n",
    "            inter = Activation('relu', name='inters.%d.inters.relu' % i)(inter)\n",
    "            inter = residual(inter, cnv_dim, 'inters.%d' % i)\n",
    "    \n",
    "    \n",
    "    flattened_output = Flatten()(outputs[-1])\n",
    "    dense = Dense(128, activation='relu')(flattened_output)\n",
    "    chest_out = Dense(1, activation='linear', name='chest_out')(dense)\n",
    "    waist_out = Dense(1, activation='linear', name='waist_out')(dense)\n",
    "    pelvis_out = Dense(1, activation='linear', name='pelvis_out')(dense)\n",
    "    bicep_out = Dense(1, activation='linear', name='bicep_out')(dense)\n",
    "    thigh_out = Dense(1, activation='linear', name='thigh_out')(dense)\n",
    "    shoulder_to_wrist_out = Dense(1, activation='linear', name='shoulder_to_wrist_out')(dense)\n",
    "    leg_out = Dense(1, activation='linear', name='leg_out')(dense)\n",
    "    calf_out = Dense(1, activation='linear', name='calf_out')(dense)\n",
    "    wrist_out = Dense(1, activation='linear', name='wrist_out')(dense)\n",
    "    shoulder_to_shoulder_out = Dense(1, activation='linear', name='shoulder_to_shoulder_out')(dense)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=[\n",
    "        chest_out,\n",
    "        waist_out,\n",
    "        pelvis_out,\n",
    "        bicep_out,\n",
    "        thigh_out,\n",
    "        shoulder_to_wrist_out,\n",
    "        leg_out,\n",
    "        calf_out,\n",
    "        wrist_out,\n",
    "        shoulder_to_shoulder_out,\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def hourglass_module(heads, bottom, cnv_dim, hgid, dims):\n",
    "    # create left features , f1, f2, f4, f8, f16 and f32\n",
    "    lfs = left_features(bottom, hgid, dims)\n",
    "\n",
    "    # create right features, connect with left features\n",
    "    rf1 = right_features(lfs, hgid, dims)\n",
    "    rf1 = convolution(rf1, 3, cnv_dim, name='cnvs.%d' % hgid)\n",
    "\n",
    "    # add 1x1 conv with two heads, inter is sent to next stage\n",
    "    # head_parts is used for intermediate supervision\n",
    "    heads = create_heads(heads, rf1, hgid)\n",
    "    return heads, rf1\n",
    "\n",
    "\n",
    "def convolution(_x, k, out_dim, name, stride=1):\n",
    "    padding = (k - 1) // 2\n",
    "    _x = ZeroPadding2D(padding=padding, name=name + '.pad')(_x)\n",
    "    _x = Conv2D(out_dim, k, strides=stride, use_bias=False, name=name + '.conv')(_x)\n",
    "    _x = BatchNormalization(epsilon=1e-5, name=name + '.bn')(_x)\n",
    "    _x = Activation('relu', name=name + '.relu')(_x)\n",
    "    return _x\n",
    "\n",
    "\n",
    "def residual(_x, out_dim, name, stride=1):\n",
    "    shortcut = _x\n",
    "    num_channels = _x.shape[-1]\n",
    "    _x = ZeroPadding2D(padding=1, name=name + '.pad1')(_x)\n",
    "    _x = Conv2D(out_dim, 3, strides=stride, use_bias=False, name=name + '.conv1')(_x)\n",
    "    _x = BatchNormalization(epsilon=1e-5, name=name + '.bn1')(_x)\n",
    "    _x = Activation('relu', name=name + '.relu1')(_x)\n",
    "\n",
    "    _x = Conv2D(out_dim, 3, padding='same', use_bias=False, name=name + '.conv2')(_x)\n",
    "    _x = BatchNormalization(epsilon=1e-5, name=name + '.bn2')(_x)\n",
    "\n",
    "    if num_channels != out_dim or stride != 1:\n",
    "        shortcut = Conv2D(out_dim, 1, strides=stride, use_bias=False, name=name + '.shortcut.0')(\n",
    "            shortcut)\n",
    "        shortcut = BatchNormalization(epsilon=1e-5, name=name + '.shortcut.1')(shortcut)\n",
    "\n",
    "    _x = Add(name=name + '.add')([_x, shortcut])\n",
    "    _x = Activation('relu', name=name + '.relu')(_x)\n",
    "    return _x\n",
    "\n",
    "\n",
    "def pre(_x, num_channels):\n",
    "    # front module, input to 1/4 resolution\n",
    "    _x = convolution(_x, 7, 128, name='pre.0', stride=2)\n",
    "    _x = residual(_x, num_channels, name='pre.1', stride=2)\n",
    "    return _x\n",
    "\n",
    "\n",
    "def left_features(bottom, hgid, dims):\n",
    "    # create left half blocks for hourglass module\n",
    "    # f1, f2, f4 , f8, f16, f32 : 1, 1/2, 1/4 1/8, 1/16, 1/32 resolution\n",
    "    # 5 times reduce/increase: (256, 384, 384, 384, 512)\n",
    "    features = [bottom]\n",
    "    for kk, nh in enumerate(dims):\n",
    "        pow_str = ''\n",
    "        for _ in range(kk):\n",
    "            pow_str += '.center'\n",
    "        _x = residual(features[-1], nh, name='kps.%d%s.down.0' % (hgid, pow_str), stride=2)\n",
    "        _x = residual(_x, nh, name='kps.%d%s.down.1' % (hgid, pow_str))\n",
    "        features.append(_x)\n",
    "    return features\n",
    "\n",
    "\n",
    "def connect_left_right(left, right, num_channels, num_channels_next, name):\n",
    "    # left: 2 residual modules\n",
    "    left = residual(left, num_channels_next, name=name + 'skip.0')\n",
    "    left = residual(left, num_channels_next, name=name + 'skip.1')\n",
    "\n",
    "    # up: 2 times residual & nearest neighbour\n",
    "    out = residual(right, num_channels, name=name + 'out.0')\n",
    "    out = residual(out, num_channels_next, name=name + 'out.1')\n",
    "    out = UpSampling2D(name=name + 'out.upsampleNN')(out)\n",
    "    out = Add(name=name + 'out.add')([left, out])\n",
    "    return out\n",
    "\n",
    "\n",
    "def bottleneck_layer(_x, num_channels, hgid):\n",
    "    # 4 residual blocks with 512 channels in the middle\n",
    "    pow_str = 'center.' * 5\n",
    "    _x = residual(_x, num_channels, name='kps.%d.%s0' % (hgid, pow_str))\n",
    "    _x = residual(_x, num_channels, name='kps.%d.%s1' % (hgid, pow_str))\n",
    "    _x = residual(_x, num_channels, name='kps.%d.%s2' % (hgid, pow_str))\n",
    "    _x = residual(_x, num_channels, name='kps.%d.%s3' % (hgid, pow_str))\n",
    "    return _x\n",
    "\n",
    "\n",
    "def right_features(leftfeatures, hgid, dims):\n",
    "    rf = bottleneck_layer(leftfeatures[-1], dims[-1], hgid)\n",
    "    for kk in reversed(range(len(dims))):\n",
    "        pow_str = ''\n",
    "        for _ in range(kk):\n",
    "            pow_str += 'center.'\n",
    "        rf = connect_left_right(leftfeatures[kk], rf, dims[kk], dims[max(kk - 1, 0)], name='kps.%d.%s' % (hgid, pow_str))\n",
    "    return rf\n",
    "\n",
    "\n",
    "def create_heads(heads, rf1, hgid):\n",
    "    _heads = []\n",
    "    for head in sorted(heads):\n",
    "        num_channels = heads[head]\n",
    "        _x = Conv2D(256, 3, use_bias=True, padding='same', name=head + '.%d.0.conv' % hgid)(rf1)\n",
    "        _x = Activation('relu', name=head + '.%d.0.relu' % hgid)(_x)\n",
    "        _x = Conv2D(num_channels, 1, use_bias=True, name=head + '.%d.1' % hgid)(_x)\n",
    "        _heads.append(_x)\n",
    "    return _heads\n"
   ],
   "id": "2f07b5bf2c60a934",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-31T21:55:08.078882Z",
     "start_time": "2024-05-31T21:55:04.125602Z"
    }
   },
   "source": [
    "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "import os\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import keras.utils\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.models import Model\n",
    "\n",
    "keras.utils.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "heads = {'regression': 10}\n",
    "model = HourglassNetwork(heads, num_stacks=2)\n",
    "\n",
    "###TRAINING VARIABLES\n",
    "\n",
    "sample_count_surreact = 100\n",
    "sample_count_bodym = 100\n",
    "epoch_count = 100\n",
    "batch_size = 1\n",
    "validation_count = 20\n",
    "initial_learning_rate=.001\n",
    "decay_steps= (sample_count_surreact + sample_count_bodym)/batch_size\n",
    "decay_rate=0.97\n",
    "model_name=\"CenterNet\"\n",
    "\n",
    "folder_path = f\"{model_name} - SR{sample_count_surreact+sample_count_bodym}s{epoch_count}e{validation_count}v {\"\" if initial_learning_rate == 0.001 else initial_learning_rate + \"ilr\"}{decay_steps}s{decay_rate}d\"\n",
    "\n",
    "try:\n",
    "    os.mkdir(os.path.join(\"./logs/\" + folder_path))\n",
    "except OSError as error:\n",
    "    print(error)\n",
    "    \n",
    "\n",
    "#model.save_weights(\"default.weights.h5\")\n",
    "# \n",
    "# model.summary()\n",
    "# \n",
    "# from keras.utils import plot_model\n",
    "# \n",
    "# plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True, show_layer_activations=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-31 23:55:04.734777: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-31 23:55:06.135567: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: './logs/CenterNet - SR200s100e20v 200.0s0.97d'\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T21:55:08.089991Z",
     "start_time": "2024-05-31T21:55:08.080193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from keras.utils import Sequence\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "\n",
    "class DatasetLoader(Sequence):\n",
    "\n",
    "    def __init__(self, image_location, data_location, samples, batch_size, dataset, shuffle=True, seed=0,\n",
    "                 input_dimensions=(240, 320), prefix=\"Avatar_\", side=False, height=False):\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "        self.input_dimensions = input_dimensions\n",
    "        self.prefix = prefix\n",
    "        self.side = side\n",
    "        self.height = height\n",
    "\n",
    "        super().__init__(workers=5, use_multiprocessing=True)\n",
    "        self.IDs = []\n",
    "        self._load_data(samples, data_location, image_location, dataset)\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.IDs) / self.batch_size))\n",
    "\n",
    "    def _load_data(self, samples, data_locations, image_locations, datasets):\n",
    "        self.IDs = []\n",
    "        self.data = dict()\n",
    "        id_counter = 0\n",
    "        for data_source_index in range(len(image_locations)):\n",
    "            source_image_count = samples[data_source_index]\n",
    "            source_data_location = data_locations[data_source_index]\n",
    "            source_image_location = image_locations[data_source_index]\n",
    "            source_dataset = datasets[data_source_index]\n",
    "\n",
    "            for index in range(source_image_count):\n",
    "                self.IDs.append(id_counter)\n",
    "                self.data[id_counter] = [source_image_location,\n",
    "                                         source_dataset,\n",
    "                                         index,\n",
    "                                         np.load(os.path.join(source_data_location + self.prefix + f\"{index:06d}.npy\"))]\n",
    "                id_counter += 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = np.empty(shape=(self.batch_size, 512, 512))\n",
    "        if self.side:\n",
    "            X = np.empty(shape=(self.batch_size, 512, 512, 2))\n",
    "        y = {\n",
    "            'chest_out': [],\n",
    "            'waist_out': [],\n",
    "            'pelvis_out': [],\n",
    "            'bicep_out': [],\n",
    "            'thigh_out': [],\n",
    "            'shoulder_to_wrist_out': [],\n",
    "            'leg_out': [],\n",
    "            'calf_out': [],\n",
    "            'wrist_out': [],\n",
    "            'shoulder_to_shoulder_out': [],\n",
    "        }\n",
    "\n",
    "        height = []\n",
    "        start_index = index * self.batch_size + 1\n",
    "        for i in range(self.batch_size):\n",
    "            sample_ID = self.IDs[(start_index + i)]\n",
    "            sample_properties = self.data[sample_ID]\n",
    "            current_measurement = sample_properties[3]\n",
    "            current_filename = f\"{self.prefix}{sample_properties[2]:06d}.png\"\n",
    "\n",
    "\n",
    "            if self.side:\n",
    "                X[i, :, :, 0] = cv2.imread(os.path.join(sample_properties[0] + \"images_front/\" + current_filename), cv2.IMREAD_GRAYSCALE)\n",
    "                X[i, :, :, 1] = cv2.imread(os.path.join(sample_properties[0] + \"images_side/\" + current_filename), cv2.IMREAD_GRAYSCALE)\n",
    "            else:\n",
    "                X[i,] = cv2.imread(os.path.join(sample_properties[0]+ \"images_front/\" + current_filename), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "            if sample_properties[1] == \"Surreact\":\n",
    "                y['chest_out'].append([current_measurement[0]])\n",
    "                y['waist_out'].append([current_measurement[1]])\n",
    "                y['pelvis_out'].append([current_measurement[2]])\n",
    "                y['bicep_out'].append([current_measurement[4]])\n",
    "                y['thigh_out'].append([current_measurement[5]])\n",
    "                y['shoulder_to_wrist_out'].append([current_measurement[7]])\n",
    "                y['leg_out'].append([current_measurement[8]])\n",
    "                y['calf_out'].append([current_measurement[9]])\n",
    "                y['wrist_out'].append([current_measurement[11]])\n",
    "                y['shoulder_to_shoulder_out'].append([current_measurement[13]])\n",
    "                if self.height:\n",
    "                    height.append(current_measurement[16])\n",
    "\n",
    "            if sample_properties[1] == \"BodyM\":\n",
    "                y['chest_out'].append([current_measurement[4]])\n",
    "                y['waist_out'].append([current_measurement[12]])\n",
    "                y['pelvis_out'].append([current_measurement[7]])\n",
    "                y['bicep_out'].append([current_measurement[2]])\n",
    "                y['thigh_out'].append([current_measurement[11]])\n",
    "                y['shoulder_to_wrist_out'].append([current_measurement[1]])\n",
    "                y['leg_out'].append([current_measurement[8]])\n",
    "                y['calf_out'].append([current_measurement[3]])\n",
    "                y['wrist_out'].append([current_measurement[13]])\n",
    "                y['shoulder_to_shoulder_out'].append([current_measurement[9]])\n",
    "                if self.height:\n",
    "                    height.append(current_measurement[6])\n",
    "\n",
    "        for key, value in y.items():\n",
    "            y[key] = np.array(value)\n",
    "\n",
    "        if self.height:\n",
    "            height = np.array(height)\n",
    "            return [X, height], y\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.IDs)\n",
    "        else:\n",
    "            self.IDs = np.arange(len(self.IDs))\n"
   ],
   "id": "3fe0cdb2e5fe4696",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T21:55:08.128670Z",
     "start_time": "2024-05-31T21:55:08.091072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "quickTrain = {\n",
    "                'image_location': [\"Export/Surreact-APoseCenterNet/train/\"], \n",
    "                'data_location': [\"Export/Surreact-APose/train/measurements/\"], \n",
    "                'samples': [sample_count_surreact, sample_count_bodym], \n",
    "                'dataset': [\"Surreact\"],\n",
    "                'batch_size': batch_size, \n",
    "                'seed':  69,#np.random.randint(0, 10000), \n",
    "                'shuffle': True}\n",
    "\n",
    "quickValidate = {\n",
    "                'image_location': [\"Export/Surreact-APoseCenterNet/test/\"],\n",
    "                'data_location': [\"Export/Surreact-APose/test/measurements/\"],\n",
    "                'samples': [validation_count],\n",
    "                'batch_size': batch_size,\n",
    "                'dataset': [\"Surreact\"],\n",
    "                'seed': 69, #np.random.randint(0, 10000),\n",
    "                }\n",
    "\n",
    "train_generator = DatasetLoader(**quickTrain)\n",
    "validation_generator = DatasetLoader(**quickValidate)"
   ],
   "id": "19d8f2f45f6a4471",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.losses import MeanSquaredError\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=True)\n",
    "\n",
    "stop_no_improvement = keras.callbacks.EarlyStopping(monitor=\"loss\", patience=3)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=lr_schedule),\n",
    "    loss={\n",
    "        'chest_out': MeanSquaredError(),\n",
    "        'waist_out': MeanSquaredError(),\n",
    "        'pelvis_out': MeanSquaredError(),\n",
    "        'bicep_out': MeanSquaredError(),\n",
    "        'thigh_out': MeanSquaredError(),\n",
    "        'shoulder_to_wrist_out': MeanSquaredError(),\n",
    "        'leg_out': MeanSquaredError(),\n",
    "        'calf_out': MeanSquaredError(),\n",
    "        'wrist_out': MeanSquaredError(),\n",
    "        'shoulder_to_shoulder_out': MeanSquaredError(),\n",
    "    },\n",
    "    metrics={\n",
    "        'chest_out': ['mae'],\n",
    "        'waist_out': ['mae'],\n",
    "        'pelvis_out': ['mae'],\n",
    "        'bicep_out': ['mae'],\n",
    "        'thigh_out': ['mae'],\n",
    "        'shoulder_to_wrist_out': ['mae'],\n",
    "        'leg_out': ['mae'],\n",
    "        'calf_out': ['mae'],\n",
    "        'wrist_out': ['mae'],\n",
    "        'shoulder_to_shoulder_out': ['mae'],\n",
    "    },\n",
    ")\n",
    "\n",
    "checkpoint_filepath = f'./models/{model_name}.checkpoint.model.keras'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "model.fit(\n",
    "    x=train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[\n",
    "\n",
    "        TensorBoard(write_graph=True, log_dir=\"./logs/\" + folder_path),\n",
    "        model_checkpoint_callback,\n",
    "        stop_no_improvement\n",
    "    ],\n",
    "    batch_size=batch_size,\n",
    "    epochs=epoch_count,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.save(f\"./models/full/{model_name}.keras\") "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T21:55:09.314473Z",
     "start_time": "2024-05-31T21:55:08.130278Z"
    }
   },
   "id": "54a15a398efa0ffb",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to automatically build the model. Please build it yourself before calling fit/evaluate/predict. A model is 'built' when its variables have been created and its `self.built` attribute is True. Usually, calling the model on a batch of data is the right way to build it.\nException encountered:\n'Exception encountered when calling Conv2D.call().\n\n\u001B[1mCUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 5.77 GiB of which 148.62 MiB is free. Process 27029 has 2.36 GiB memory in use. Including non-PyTorch memory, this process has 2.30 GiB memory in use. Of the allocated memory 2.16 GiB is allocated by PyTorch, and 8.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\u001B[0m\n\nArguments received by Conv2D.call():\n  • inputs=torch.Tensor(shape=torch.Size([1, 34, 34, 384]), dtype=float32)'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 48\u001B[0m\n\u001B[1;32m     41\u001B[0m checkpoint_filepath \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./models/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.checkpoint.model.keras\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     42\u001B[0m model_checkpoint_callback \u001B[38;5;241m=\u001B[39m keras\u001B[38;5;241m.\u001B[39mcallbacks\u001B[38;5;241m.\u001B[39mModelCheckpoint(\n\u001B[1;32m     43\u001B[0m     filepath\u001B[38;5;241m=\u001B[39mcheckpoint_filepath,\n\u001B[1;32m     44\u001B[0m     monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     45\u001B[0m     mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmin\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     46\u001B[0m     save_best_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 48\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     49\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_generator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     50\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_generator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\n\u001B[1;32m     52\u001B[0m \n\u001B[1;32m     53\u001B[0m \u001B[43m        \u001B[49m\u001B[43mTensorBoard\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwrite_graph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./logs/\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mfolder_path\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel_checkpoint_callback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstop_no_improvement\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[43m    \u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepoch_count\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\n\u001B[1;32m     60\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     62\u001B[0m model\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./models/full/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.keras\u001B[39m\u001B[38;5;124m\"\u001B[39m) \n",
      "File \u001B[0;32m~/School/DomainAdaptation/BoMN/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m    120\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m--> 122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/School/DomainAdaptation/BoMN/.venv/lib/python3.12/site-packages/keras/src/trainers/trainer.py:924\u001B[0m, in \u001B[0;36mTrainer._symbolic_build\u001B[0;34m(self, iterator, data_batch)\u001B[0m\n\u001B[1;32m    922\u001B[0m     y_pred \u001B[38;5;241m=\u001B[39m backend\u001B[38;5;241m.\u001B[39mcompute_output_spec(\u001B[38;5;28mself\u001B[39m, x)\n\u001B[1;32m    923\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 924\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    925\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to automatically build the model. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    926\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease build it yourself before calling \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    927\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit/evaluate/predict. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    928\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mA model is \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbuilt\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m when its variables have \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    929\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbeen created and its `self.built` attribute \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    930\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis True. Usually, calling the model on a batch \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    931\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mof data is the right way to build it.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    932\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mException encountered:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    933\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    934\u001B[0m     )\n\u001B[1;32m    935\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m compile_metrics_unbuilt:\n\u001B[1;32m    936\u001B[0m     \u001B[38;5;66;03m# Build all metric state with `backend.compute_output_spec`.\u001B[39;00m\n\u001B[1;32m    937\u001B[0m     backend\u001B[38;5;241m.\u001B[39mcompute_output_spec(\n\u001B[1;32m    938\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_metrics,\n\u001B[1;32m    939\u001B[0m         x,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    942\u001B[0m         sample_weight\u001B[38;5;241m=\u001B[39msample_weight,\n\u001B[1;32m    943\u001B[0m     )\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Unable to automatically build the model. Please build it yourself before calling fit/evaluate/predict. A model is 'built' when its variables have been created and its `self.built` attribute is True. Usually, calling the model on a batch of data is the right way to build it.\nException encountered:\n'Exception encountered when calling Conv2D.call().\n\n\u001B[1mCUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 5.77 GiB of which 148.62 MiB is free. Process 27029 has 2.36 GiB memory in use. Including non-PyTorch memory, this process has 2.30 GiB memory in use. Of the allocated memory 2.16 GiB is allocated by PyTorch, and 8.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\u001B[0m\n\nArguments received by Conv2D.call():\n  • inputs=torch.Tensor(shape=torch.Size([1, 34, 34, 384]), dtype=float32)'"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
