{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-31T12:55:45.019806Z",
     "start_time": "2024-03-31T12:55:42.865319Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras.utils\n",
    "import numpy as np\n",
    "import tqdm.notebook\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "inp = Input(shape=(240, 320, 1))\n",
    "\n",
    "n_filters = 512\n",
    "max_p = inp\n",
    "\n",
    "for _ in range(3):\n",
    "    conv = Conv2D(filters=n_filters, kernel_size=5, activation='relu')(max_p)\n",
    "    max_p = MaxPooling2D(pool_size=(2, 2))(conv)\n",
    "    \n",
    "    n_filters //= 2\n",
    "\n",
    "conv = Conv2D(filters=n_filters, kernel_size=3, activation='relu')(max_p)\n",
    "max_p = MaxPooling2D(pool_size=(2, 2))(conv)\n",
    "\n",
    "flatten = Flatten()(max_p)\n",
    "dense = Dense(128, activation='relu')(flatten)\n",
    "# out = Dense(14, activation='relu')(dense)\n",
    "\n",
    "head_out = Dense(1, activation='relu', name='head_out')(dense)\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    inputs=inp,\n",
    "    outputs=[\n",
    "    head_out\n",
    "    ],\n",
    "    name='head_only'\n",
    ")\n",
    "\n",
    "#model.save_weights(\"default.weights.h5\")\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "class Database_Loader(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, image_location, data_location, sample_count, batch_size, shuffle=True, seed=0, input_dimensions=(240, 320), prefix=\"Avatar_\", use_memory=False, load_data = True, random_sample = False):\n",
    "        self.image_location = image_location\n",
    "        self.data_location = data_location\n",
    "        self.sample_count = sample_count\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "        self.input_dimensions = input_dimensions\n",
    "        self.prefix = prefix\n",
    "        self.use_memory = use_memory\n",
    "        self.load_data = load_data\n",
    "        self.random_sample = random_sample\n",
    "\n",
    "        if self.random_sample:\n",
    "            self.IDs = random.sample(range(len(os.listdir(self.image_location))),self.sample_count)\n",
    "        else:\n",
    "            self.IDs = [x for x in range(self.sample_count)]\n",
    "            self._load_data()\n",
    "\n",
    "        super().__init__(workers=2, use_multiprocessing=True)\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.floor(self.sample_count / self.batch_size))\n",
    "    \n",
    "    \n",
    "    def _load_data(self):\n",
    "        if self.load_data:\n",
    "            self.data = dict()\n",
    "            for index in self.IDs:\n",
    "                self.data[index] = np.load(os.path.join(self.data_location + self.prefix + f\"{index:06d}.npy\"))\n",
    "\n",
    "        if self.use_memory:\n",
    "            self.images = dict()\n",
    "            for index in self.IDs:\n",
    "                self.images[index] = cv2.imread(os.path.join(self.image_location + self.prefix + f\"{index:06d}.png\"), cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = np.empty(shape=(self.batch_size, 240, 320))\n",
    "        y = {\n",
    "            'head_out': [],\n",
    "        }\n",
    "        \n",
    "        start_index = index * self.batch_size + 1\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            name = f\"{self.IDs[(start_index + i)%self.sample_count]:06d}\"\n",
    "            \n",
    "            if self.load_data:\n",
    "                current_measurement = self.data[self.IDs[(start_index + i)%self.sample_count]][:-1]          \n",
    "            else:\n",
    "                current_measurement = np.load(os.path.join(self.data_location + self.prefix + name + \".npy\"))[:-1]\n",
    "                \n",
    "            if self.use_memory:\n",
    "                X[i,] = self.images[self.IDs[(start_index + i)%self.sample_count]]\n",
    "                \n",
    "            else:\n",
    "                X[i,] = cv2.imread(os.path.join(self.image_location + self.prefix + name + \".png\"), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "            y['head_out'].append([current_measurement[0]])\n",
    "\n",
    "        for key, value in y.items():\n",
    "            y[key] = np.array(value)\n",
    "            \n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.random_sample:\n",
    "            self.IDs = random.sample(range(len(os.listdir(self.image_location))),self.sample_count)\n",
    "            self._load_data()\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.seed(self.seed)\n",
    "            np.random.shuffle(self.IDs)\n",
    "        else:\n",
    "            self.IDs = np.arange(self.sample_count)\n",
    "            \n",
    "    \n",
    "    \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T12:55:45.150959Z",
     "start_time": "2024-03-31T12:55:45.021233Z"
    }
   },
   "id": "e9bd08ef645fecfa",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "41ca0e59230d49eba1ac93f49b35f2ec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# quickTrain = {'image_location': \"../Surreact-APose/train/imgs_nobg_frontEDITED/\", \n",
    "#               'data_location': \"../Surreact-APose/train/measurements/\", \n",
    "#               'sample_count': 5000, \n",
    "#               'batch_size': 4, \n",
    "#               'seed': 69, #np.random.randint(0, 10000), \n",
    "#               'use_memory': False,\n",
    "#               'random_sample': True}\n",
    "# \n",
    "# quickValidate = {'image_location': \"../Surreact-APose/train/imgs_nobg_frontEDITED/\",\n",
    "#               'data_location': \"../Surreact-APose/train/measurements/\",\n",
    "#               'sample_count': 1000,\n",
    "#               'batch_size': 4,\n",
    "#               'seed': 69, #np.random.randint(0, 10000),\n",
    "#               'use_memory': False,\n",
    "#               'random_sample': True}\n",
    "# \n",
    "# train_generator = Database_Loader(**quickTrain)\n",
    "# validation_generator = Database_Loader(**quickValidate)\n",
    "\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "X = []\n",
    "y_true = []\n",
    "\n",
    "for i in tqdm.notebook.tqdm(range(1000)):\n",
    "    X.append(cv2.imread(os.path.join(f\"../Surreact-APose/train/imgs_nobg_frontEDITED/Avatar_000000.png\"), cv2.IMREAD_GRAYSCALE))\n",
    "    y_true.append(np.load(os.path.join(f\"../Surreact-APose/train/measurements/Avatar_000000.npy\"))[0])\n",
    "\n",
    "\n",
    "X = np.array(X)\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "# X = np.array([data[0] for data in dataset.values()])\n",
    "# y_true = np.array([data[1] for data in dataset.values()])\n",
    "# \n",
    "# (\n",
    "#     y_true_head, y_true_neck, y_true_sts,y_true_arm, y_true_stw, y_true_torso, y_true_bicep, y_true_wrist, y_true_chest, y_true_waist, y_true_pelvis, y_true_leg, y_true_inner_leg, y_true_thigh, y_true_knee, y_true_calf, height\n",
    "# ) = np.hsplit(y_true, 17) #17 is height\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T12:55:46.131955Z",
     "start_time": "2024-03-31T12:55:45.151825Z"
    }
   },
   "id": "19d8f2f45f6a4471",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9d364bdaa0fb5da4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-31 14:55:56.316271: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-31 14:55:57.410937: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-31 14:55:57.964691: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2024-03-31 14:55:57.964896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3352 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m366/800\u001B[0m \u001B[32m━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━\u001B[0m \u001B[1m25s\u001B[0m 60ms/step - loss: 0.8881"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 23\u001B[0m\n\u001B[1;32m     15\u001B[0m checkpoint_filepath \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./models/checkpoint.model.keras\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     16\u001B[0m model_checkpoint_callback \u001B[38;5;241m=\u001B[39m keras\u001B[38;5;241m.\u001B[39mcallbacks\u001B[38;5;241m.\u001B[39mModelCheckpoint(\n\u001B[1;32m     17\u001B[0m     filepath\u001B[38;5;241m=\u001B[39mcheckpoint_filepath,\n\u001B[1;32m     18\u001B[0m     monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     19\u001B[0m     mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmin\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     20\u001B[0m     save_best_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 23\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     25\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_split\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[43m        \u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[43m        \u001B[49m\u001B[43mTensorBoard\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwrite_graph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43mlog_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./logs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# TQDMNotebookCallback(separator=\"\", metric_format=\"{name}: {value:0.8f} || \")\u001B[39;49;00m\n\u001B[1;32m     31\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# model_checkpoint_callback\u001B[39;49;00m\n\u001B[1;32m     32\u001B[0m \u001B[43m    \u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     33\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     34\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m200\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[43m    \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[1;32m     37\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# \u001B[39;00m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# model.fit(\u001B[39;00m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m#     x=X,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;66;03m#     callbacks=[TensorBoard(write_graph=False)]\u001B[39;00m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;66;03m# )\u001B[39;00m\n",
      "File \u001B[0;32m~/School/DomainAdaptation/BoMN/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:118\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    116\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 118\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    120\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/School/DomainAdaptation/BoMN/.venv/lib/python3.12/site-packages/keras/src/backend/torch/trainer.py:249\u001B[0m, in \u001B[0;36mTorchTrainer.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001B[0m\n\u001B[1;32m    245\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step, data \u001B[38;5;129;01min\u001B[39;00m epoch_iterator\u001B[38;5;241m.\u001B[39menumerate_epoch():\n\u001B[1;32m    246\u001B[0m     \u001B[38;5;66;03m# Callbacks\u001B[39;00m\n\u001B[1;32m    247\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[0;32m--> 249\u001B[0m     logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    251\u001B[0m     \u001B[38;5;66;03m# Callbacks\u001B[39;00m\n\u001B[1;32m    252\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_end(step, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pythonify_logs(logs))\n",
      "File \u001B[0;32m~/School/DomainAdaptation/BoMN/.venv/lib/python3.12/site-packages/keras/src/backend/torch/trainer.py:112\u001B[0m, in \u001B[0;36mTorchTrainer.make_train_function.<locals>.one_step_on_data\u001B[0;34m(data)\u001B[0m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001B[39;00m\n\u001B[1;32m    111\u001B[0m data \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m--> 112\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/School/DomainAdaptation/BoMN/.venv/lib/python3.12/site-packages/keras/src/backend/torch/trainer.py:51\u001B[0m, in \u001B[0;36mTorchTrainer.train_step\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;66;03m# Call torch.nn.Module.zero_grad() to clear the leftover gradients\u001B[39;00m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;66;03m# for the weights from the previous train step.\u001B[39;00m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 51\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_loss_tracker\u001B[38;5;241m.\u001B[39mupdate_state(loss)\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/School/DomainAdaptation/BoMN/.venv/lib/python3.12/site-packages/keras/src/trainers/trainer.py:321\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[0;34m(***failed resolving arguments***)\u001B[0m\n\u001B[1;32m    319\u001B[0m losses \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    320\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compile_loss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 321\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_compile_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    322\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m loss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    323\u001B[0m         losses\u001B[38;5;241m.\u001B[39mappend(loss)\n",
      "File \u001B[0;32m~/School/DomainAdaptation/BoMN/.venv/lib/python3.12/site-packages/keras/src/trainers/compile_utils.py:606\u001B[0m, in \u001B[0;36mCompileLoss.__call__\u001B[0;34m(self, y_true, y_pred, sample_weight)\u001B[0m\n\u001B[1;32m    604\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, y_true, y_pred, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    605\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ops\u001B[38;5;241m.\u001B[39mname_scope(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname):\n\u001B[0;32m--> 606\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/School/DomainAdaptation/BoMN/.venv/lib/python3.12/site-packages/keras/src/trainers/compile_utils.py:642\u001B[0m, in \u001B[0;36mCompileLoss.call\u001B[0;34m(self, y_true, y_pred, sample_weight)\u001B[0m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m loss, y_t, y_p, loss_weight, sample_weight \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\n\u001B[1;32m    634\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mflat_losses,\n\u001B[1;32m    635\u001B[0m     y_true,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    638\u001B[0m     sample_weight,\n\u001B[1;32m    639\u001B[0m ):\n\u001B[1;32m    640\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m loss:\n\u001B[1;32m    641\u001B[0m         value \u001B[38;5;241m=\u001B[39m loss_weight \u001B[38;5;241m*\u001B[39m ops\u001B[38;5;241m.\u001B[39mcast(\n\u001B[0;32m--> 642\u001B[0m             \u001B[43mloss\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_t\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_p\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m)\u001B[49m, dtype\u001B[38;5;241m=\u001B[39mbackend\u001B[38;5;241m.\u001B[39mfloatx()\n\u001B[1;32m    643\u001B[0m         )\n\u001B[1;32m    644\u001B[0m         loss_values\u001B[38;5;241m.\u001B[39mappend(value)\n\u001B[1;32m    645\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m loss_values:\n",
      "File \u001B[0;32m~/School/DomainAdaptation/BoMN/.venv/lib/python3.12/site-packages/keras/src/losses/loss.py:55\u001B[0m, in \u001B[0;36mLoss.__call__\u001B[0;34m(self, y_true, y_pred, sample_weight)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m---> 55\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mreduce_weighted_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlosses\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     60\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     61\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/School/DomainAdaptation/BoMN/.venv/lib/python3.12/site-packages/keras/src/losses/loss.py:152\u001B[0m, in \u001B[0;36mreduce_weighted_values\u001B[0;34m(values, sample_weight, mask, reduction, dtype)\u001B[0m\n\u001B[1;32m    149\u001B[0m     values \u001B[38;5;241m=\u001B[39m values \u001B[38;5;241m*\u001B[39m sample_weight\n\u001B[1;32m    151\u001B[0m \u001B[38;5;66;03m# Apply reduction function to the individual weighted losses.\u001B[39;00m\n\u001B[0;32m--> 152\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mreduce_values\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m~/School/DomainAdaptation/BoMN/.venv/lib/python3.12/site-packages/keras/src/losses/loss.py:117\u001B[0m, in \u001B[0;36mreduce_values\u001B[0;34m(values, reduction)\u001B[0m\n\u001B[1;32m    114\u001B[0m loss \u001B[38;5;241m=\u001B[39m ops\u001B[38;5;241m.\u001B[39msum(values)\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m reduction \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msum_over_batch_size\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    116\u001B[0m     loss \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m=\u001B[39m ops\u001B[38;5;241m.\u001B[39mcast(\n\u001B[0;32m--> 117\u001B[0m         ops\u001B[38;5;241m.\u001B[39mprod(\u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_to_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mint32\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m),\n\u001B[1;32m    118\u001B[0m         loss\u001B[38;5;241m.\u001B[39mdtype,\n\u001B[1;32m    119\u001B[0m     )\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m~/School/DomainAdaptation/BoMN/.venv/lib/python3.12/site-packages/keras/src/ops/core.py:493\u001B[0m, in \u001B[0;36mconvert_to_tensor\u001B[0;34m(x, dtype, sparse)\u001B[0m\n\u001B[1;32m    474\u001B[0m \u001B[38;5;129m@keras_export\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkeras.ops.convert_to_tensor\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    475\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconvert_to_tensor\u001B[39m(x, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, sparse\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    476\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Convert a NumPy array to a tensor.\u001B[39;00m\n\u001B[1;32m    477\u001B[0m \n\u001B[1;32m    478\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    491\u001B[0m \u001B[38;5;124;03m    >>> y = keras.ops.convert_to_tensor(x)\u001B[39;00m\n\u001B[1;32m    492\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 493\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mbackend\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_to_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/School/DomainAdaptation/BoMN/.venv/lib/python3.12/site-packages/keras/src/backend/torch/core.py:203\u001B[0m, in \u001B[0;36mconvert_to_tensor\u001B[0;34m(x, dtype, sparse)\u001B[0m\n\u001B[1;32m    199\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m result_type(\n\u001B[1;32m    200\u001B[0m         \u001B[38;5;241m*\u001B[39m[\u001B[38;5;28mgetattr\u001B[39m(item, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mtype\u001B[39m(item)) \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m tree\u001B[38;5;241m.\u001B[39mflatten(x)]\n\u001B[1;32m    201\u001B[0m     )\n\u001B[1;32m    202\u001B[0m dtype \u001B[38;5;241m=\u001B[39m to_torch_dtype(dtype)\n\u001B[0;32m--> 203\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mas_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mget_device\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.losses import MeanSquaredError\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "model.load_weights(\"default.weights.h5\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='mse',\n",
    ")\n",
    "keras.utils.set_random_seed(42)\n",
    "\n",
    "\n",
    "checkpoint_filepath = './models/checkpoint.model.keras'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    x=X,\n",
    "    y=y_true,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[\n",
    "        \n",
    "        TensorBoard(write_graph=False,log_dir=\"./logs\"),\n",
    "        # TQDMNotebookCallback(separator=\"\", metric_format=\"{name}: {value:0.8f} || \")\n",
    "        # model_checkpoint_callback\n",
    "    ],\n",
    "    batch_size=1,\n",
    "    epochs=200,\n",
    "    verbose=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# \n",
    "# model.fit(\n",
    "#     x=X,\n",
    "#     y={\n",
    "#         'head_out': y_true_head,\n",
    "#         'neck_out': y_true_neck,\n",
    "#         'shoulder_to_shoulder_out': y_true_sts,\n",
    "#         'arm_out': y_true_arm,\n",
    "#         'shoulder_to_wrist_out': y_true_stw,\n",
    "#         'torso_out': y_true_torso,\n",
    "#         'bicep_out': y_true_bicep,\n",
    "#         'wrist_out': y_true_wrist,\n",
    "#         'chest_out': y_true_chest,\n",
    "#         'waist_out': y_true_waist,\n",
    "#         'pelvis_out': y_true_pelvis,\n",
    "#         'leg_out': y_true_leg,\n",
    "#         'inner_leg_out': y_true_inner_leg,\n",
    "#         'thigh_out': y_true_thigh,\n",
    "#         'knee_out': y_true_knee,\n",
    "#         'calf_out': y_true_calf\n",
    "#     },\n",
    "#     batch_size=8,\n",
    "#     epochs=10,\n",
    "#     validation_data=train_data[1],\n",
    "#     shuffle=True,\n",
    "#     callbacks=[TensorBoard(write_graph=False)]\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T12:56:20.608836Z",
     "start_time": "2024-03-31T12:55:56.044756Z"
    }
   },
   "id": "e2e37a391e152295",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "    #Data modifiaction\n",
    "\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "# \n",
    "# avatar_count = 79999\n",
    "# counter = 0\n",
    "# \n",
    "# for number in tqdm(range(avatar_count)):\n",
    "#     if number == 5:\n",
    "#         continue\n",
    "#     file = (tuple(np.load(f\"../Surreact-APose/train/measurements/Avatar_{number:06d}.npy\", allow_pickle=True)))\n",
    "#     measurements = []\n",
    "#     for value in file:\n",
    "#         measurements.append(float(value))\n",
    "#     np.save(f\"../Surreact-APose/train/measurements/Avatar_{counter:06d}\", measurements)\n",
    "#     counter += 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6aa9583a841fdb3f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Image modification\n",
    "\n",
    "# import os\n",
    "# import cv2\n",
    "# from tqdm import tqdm\n",
    "# \n",
    "# \n",
    "# avatar_count = 79999\n",
    "# counter = 0\n",
    "# \n",
    "# for number in tqdm(range(avatar_count)):\n",
    "#     if number == 5:\n",
    "#         continue\n",
    "#     file = cv2.imread(f\"../Surreact-APose/train/imgs_nobg_front/Avatar_{number:06d}.png\", cv2.IMREAD_UNCHANGED)\n",
    "#     file = cv2.cvtColor(file, cv2.COLOR_mRGBA2RGBA)\n",
    "#     ret, binary = cv2.threshold(file, 0, 255, 0)\n",
    "# \n",
    "#     cv2.imwrite(f\"../Surreact-APose/train/imgs_nobg_frontEDITED/Avatar_{counter:06d}.png\", binary)\n",
    "#     counter +=1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df9f18fda9a0a122",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# from tensorboard.backend.event_processing import event_accumulator\n",
    "# import pandas as pd\n",
    "# \n",
    "# \n",
    "# def parse_tensorboard(path, scalars):\n",
    "#     \"\"\"returns a dictionary of pandas dataframes for each requested scalar\"\"\"\n",
    "#     ea = event_accumulator.EventAccumulator(\n",
    "#         path,\n",
    "#         size_guidance={event_accumulator.SCALARS: 0},\n",
    "#     )\n",
    "#     _absorb_print = ea.Reload()\n",
    "#     print(ea.Tags()[\"tensors\"])\n",
    "#     # make sure the scalars are in the event accumulator tags\n",
    "#     assert all(\n",
    "#         s in ea.Tags()[\"tensors\"] for s in scalars\n",
    "#     ), \"some scalars were not found in the event accumulator\"\n",
    "#     return {k: pd.DataFrame(ea.Tensors(k)) for k in scalars}\n",
    "# \n",
    "# \n",
    "# \n",
    "# a = parse_tensorboard(\"logsTest/train/events.out.tfevents.1711732161.root.21184.39.v2\",['epoch_head_out_mae', 'batch_head_out_mae'])\n",
    "# \n",
    "# \n",
    "# print(a)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ebb8a9694a66d73",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b856b517919a5ab5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
