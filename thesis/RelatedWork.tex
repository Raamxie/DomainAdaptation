\chapter{Related Work}
%\section{Automatic Estimation of Anthropometric Human Body Measurements}
%This thesis builds on foundations laid out by article from \cite{super}. This article delves into estimating body measurements on different data types. As this thesis works with 2D data only, we will use only relevant parts. Moreover, the article provides us with annotations for synthetic dataset by Surreact \cite{surreact}. 
The need of knowing human measurements is always present. The methods used for obtaining have been developing and nowadays are competing with traditional measurement methods in precision. The progress has been developing also thanks to new ways of obtaining data used for these estimations. First, only numerical values were available to researchers. Available measurements were used for statistical models which were used to roughly estimate average measurements.


%The solution proposed in \cite{HBDE1} provides us with a method which estimates subject's height using single uncalibrated image. Another approach  \cite{KeepItSMPL} uses joints position estimation to create a 3D mesh used for further evaluation.
\section{History}
Quetelet's concept of the average man has had a profound influence on the development of psychology and the statistical study of human characteristics~\cite{adolphe}. Quetelet argued that measurements of human traits would conform to the normal distribution, with the average representing the true or ideal type. This notion of the average as representative allowed early psychologists to blur the distinction between individual-level data and aggregate statistics.
The idea of the average man as a statistical model for understanding human nature persisted in psychology, even as reporting practices shifted away from individual-level results towards aggregate statistics. Quetelet's work laid the groundwork for the widespread use of large-scale data collection and analysis techniques, which became central to the field's pursuit of understanding individual differences and population-level phenomena.
The legacy of Quetelet's work highlights the important epistemic challenges that arise when connecting statistical models to claims about individuals and human nature. This historical context is relevant for understanding the development of methods for estimating human body measurements from data, which often rely on aggregate statistics and population-level modeling.

Since the 18th century, the military began employing anthropometric measurements, primarily focusing on stature, to identify suitable candidates~\cite{anthrohistory}.
...
\section{Pose Estimation}
Pose estimation is a computer vision task that involves detecting and locating key points on objects or persons in images or videos. These key points correspond to specific body parts or landmarks, such as joints. The goal is to accurately determine the spatial position and orientation (pose) of the person within the image or video frame. Pose estimation has various applications, including human-computer interaction, gesture recognition, action recognition, augmented reality, and robotics. It's a fundamental technology that enables machines to understand and interact with the physical world more intuitively. 








\section{Domain Adaptation}
When training a machine learning model, such as a neural network, it is crucial for the performance of the model that the training and testing data are similar and follow the same distribution~\cite{domainAdaptation}. However, this condition is often not met. One reason for this is the limited amount of training data, as demonstrated in this thesis. Another reason is the slight differences between the test data and the data the network was trained on. For instance, this can occur with medical devices where output devices may have varying colour representations. Considering factors like time complexity and data availability, domain adaptation can be an optimal solution to address these discrepancies.

Domain adaptation is a type of transfer learning, used to mitigate domain shift between source (training data) and target (data used for testing) domain.  The assumption behind domain adaptation is, that there exists a difference in domains (source and target), but not in the task. Furthermore, the domains should not be too different and should have similar probability distributions. These limitations impact the performance, up to point where network's performance may be worse than before applying domain adaptation if used incorrectly. On the other hand, there are multiple advantages to using domain adaptation. For example, the cost of collecting and labelling new data can be mitigated. In addition, the resulting network is able to generalise better on the target domain. Multiple domain adaptation approaches have been developed to address this issue:




\subsection{Instance-Based Methods}
 Instance-based methods reweigh or select instances from the source domain to better match the target domain's distribution. One of the notable approaches, Kernel Mean Matching~\cite{instanceMethod} (or KMM for short),  aims to adjust the distribution of the source domain by assigning weights to source instances. This process minimizes the difference between the  source and target distributions in a high-dimensional feature space.
 
 \subsection{Feature-Based Methods}
 Feature-based methods focus on learning a common feature representation that is invariant across domains. These methods typically involve transforming the feature space such that the source and target domains become indistinguishable. One important method is the Maximum Mean Discrepancy (MMD). DAN~\cite{featureMethod} incorporates MMD into a deep neural network, aligning the distributions of source and target features at multiple layers of the network. %Explain MMD
 
 Another influential method is Domain-Adversarial Neural Networks (DANN)~\cite{featureMethod2}. DANN employs adversarial training, where a domain classifier is trained to be able to distinguish source and target features while the feature extractor simultaneously learns to confuse the domain classifier. This adversarial process influences the feature extractor to generate domain-invariant features, enhancing model performance on the target domain.
 
 \subsection{Parameter-Based Methods}
 Parameter-based methods involve sharing or regularizing model parameters between the source and target domains. These methods often adapt the source model to the target domain by imposing regularization constraints. Deep Domain Confusion (DDC)~\cite{parameterMethod} combines a domain confusion loss with the standard classification loss. The domain confusion loss encourages the network to learn features that are not only discriminative for the source task but also invariant across domains. %needs better explanation what it really does

\subsection{Hybrid Methods}
Hybrid methods integrate multiple adaptation strategies to leverage their complementary strengths. CyCADA (Cycle-Consistent Adversarial Domain Adaptation)~\cite{hybridMethod} combines adversarial training, cycle-consistency, and feature alignment. CyCADA uses generative adversarial networks (GANs) to translate images from the source domain to the target domain and vice versa, ensuring that the translations are cycle-consistent. This approach helps align the feature distributions while preserving the semantic content, leading to improved adaptation performance.

CyCADA uses principles proposed by CycleGAN~\cite{CycleGAN}. Firstly, it employs a generators and discriminators to create cycle consistency. The generators are used to map images from the source domain to the target domain and then back to the original domain.










\section{Data synthesis}
Neural networks have caused a shift in task solving in numerous fields. These tasks can be rather complex and require a large quantity of quality prepared datasets. To create such dataset, the data has to be collected and further correctly labelled. This copious task is however not so simple. There are many obstacles that have to be overcome to create usable dataset, of which some are:

\paragraph{Cost} Creating datasets can be expensive as they have to be large, often requiring multiple workers to collect and label data. In some cases special devices and environment may be required to be able to obtain such data, which further increases the cost.
\paragraph{Time complexity} As the type of data vary from task to task, the data may be complicated to obtain, such as taking photographs in different regions or measuring values using complex machinery or many others. The data collection is, however, only half of the task. The other half consists of correctly labelling the data. Depending upon the task, it can sometimes require professionals to assess the data.
\paragraph{Privacy} In many medical applications the task requires personal data to make correct predictions. This raises concerns about private information leaking and thus the data has to be dealt with in way it follows all privacy regulations. Moreover, there must be enough willing patients to be able to procure such dataset, which can raise the monetary cost of such dataset. Medicine is not the only field in which personal information may be required to create suitable dataset.
\paragraph{Robustness} In order for the network to be able to correctly generalise, the dataset should be as general as is possible, including extreme and unusual cases as well. If this is not the case, the network may struggle with such cases or can develop a bias to the most common example. This is often the most complicated task, and it is rarely able to fully contain the whole spectrum.
\paragraph{Data availability} Not all data is available all the time and thus obtaining can be impossible at that specific time. Most commonly this issue rises when the subject of the dataset is rare (for example certain illnesses or animals etc.) or is currently not available (wrong season or not appropriate conditions).

\subsection{Synthetic data}
To counteract these obstacles, some data can be created synthetically. This type of data is artificially generated instead of collected manually. The generation is based on models and algorithms which aim to create samples that are adjusted to meet specific requirements, but also contain features and variation the real data may be missing. The main aim of this data is to make the network more robust, helping it to be precise even in situations that were not included in the real data \cite{synthDataRobust}. Other than better precision, generating data can be far less expensive than collecting it. 

There are currently multiple approaches that are used for generation of synthetic data~\cite{synthDataReview}. We will present some methods that could be used with issue


\section{Neural Anthropometer}
An important article is the~\cite{source} which proposes a method to tackle this task. Its Neural Anthropometer provides a valuable approach which will we use as backbone our convolutional neural network architecture. We do not need everything used in this article as we already have annotated synthetic dataset provided by~\cite{super}. To keep the network as small as possible due to resource consumption and training difficulty increase with size. The proposed architecture starts with a binary image silhouette input. This is then processed by a convolutional layer. Number of channels was based on number of values on output. The output tensor is then passed through ReLU~\cite{relu} along with batch optimization. Subsequently, a max pooling layer is used followed by a  convolutional layer. The output is then once again processed by max pooling layer with configuration same as before.  The result is then flattened to a tensor which is passed to a fully connected layer and a ReLU. As the last layer a regressor is used to provide the measurements estimation.

This approach is further implemented by~\cite{super}. The article delves into different data representations of human bodies and influence this 
