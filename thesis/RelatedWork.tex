\chapter{Related Work}

The need to understand human measurements has always been essential. The methods for obtaining these measurements have continually evolved, and today, they rival traditional techniques in precision. This progress has been driven by innovative ways of collecting data for these estimations.

%The solution proposed in \cite{HBDE1} provides us with a method which estimates subject's height using single uncalibrated image. Another approach  \cite{KeepItSMPL} uses joints position estimation to create a 3D mesh used for further evaluation.
\section{History of Anthropometry}
Quetelet's concept of the "average man"~\cite{QUETELET} has profoundly influenced the development of psychology and the statistical study of human characteristics. Quetelet argued that measurements of human traits would conform to a normal distribution, with the average representing the true or ideal type. This notion of the average as representative allowed early psychologists to blur the distinction between individual-level data and aggregate statistics. Success of this idea cannot be diminished as one of the outcomes of this the idea Body Mass Index (BMI) is still used to this day~\cite{bmiUsage}. The idea of the average man as a statistical model for understanding human nature persisted in psychology, even as reporting practices shifted towards aggregate statistics. Quetelet's work laid the groundwork for the widespread use of large-scale data collection and analysis techniques, which became central to understanding individual differences and population-level phenomena. 

The legacy of Quetelet's work highlights the important epistemic challenges that arise when connecting statistical models to claims about individuals and human nature. This historical context is relevant for understanding the development of methods for estimating human body measurements from data, which often rely on aggregate statistics and population-level modeling.

Another significant impact of anthropometry on human society is illustrated by the Bertillon system~\cite{sus}. In the late 1870s, the French policeman Alphonse Bertillon began measuring prisoners for later identification. This approach helped in identifying recidivists and provided the police with additional information.

In the early 20th century, anthropometric measurements took on a new function. Predictions of body fat were based on measurements such as body length, circumference, and skinfold thickness~\cite{anthroHist2}. This method spread rapidly due to its non-invasiveness, portability, and cost-effectiveness.

Nowadays, anthropometric measurements remain important in multiple fields. Digital anthropometry is being used to measure the size of infants' bodies to check for nutrition and growth anomalies~\cite{nutrition}. Additionally, companies like Treedy's~\cite{treedys} are incorporating digital measuring stations in clothing stores to help customers pick the correct clothing size and even recommend fitting pieces.

\begin{figure}
	\centering{\includegraphics[scale=0.25]{images/Treedys}}
	\caption[Treedy's scanning device]{Treedy's body scanning station used in Decathlon stores. Customers can obtain their body measurements just by walking in. The device scans them using four camera-equipped devices and, using their patented Nakednet AI, estimates the measurements. Credit: Treedy's~\cite{treedys}.}
	\label{treedys}
\end{figure}

\section{Types of Human Body Estimations}
\subsection{Mathematical Models}
Calculating the Body Mass Index (BMI) from body dimensions~\cite{bmiPredict} became crucial in tracking the development of obesity pandemic during the 20th century~\cite{bmiUsage}. This led researchers to develop statistical models to detect overweight or underweight conditions in the population. The measurements required for these tasks varied, but the popular approach resulted in requiring only height and weight~\cite{bmiHW} based on the BMI formula~\cite{bmi}.

Another significant task involved determining the centre of mass of the human body, the moment of inertia, and related parameters, including BMI. These measurements are essential for various applications, such as Self-Maneuvering Units~\cite{bodyInertia} used for extravehicular movement in space or the development of prosthetic devices and man-machine interfaces~\cite{bodyInertia2}. These tasks continue to be researched today~\cite{bodyMotion}, highlighting the ongoing need for such studies.

In anthropology, an anatomical task arose to estimate human stature from parts of the skeleton~\cite{skeletonHeight}. Researchers have used various indicators, such as brain weight~\cite{brainStature}, femur size~\cite{femurStature}, and more~\cite{statureHistory}, to estimate stature, aiding scientists in understanding the shape, evolution, and environmental influences on humanity.
\subsection{Regression}
As powerful computational devices became available to scientists, more complex models of regression analysis have enabled more precise estimations. Over the years the popularity grew not only for precision of results, but also thanks to the ubiquity and availability  of large datasets~\cite{regressionStature}. In recent years it has become an essential part of statistics for students of anthropology~\cite{regressionStudents}.

Regression methods often build on the same hypotheses as mathematical models. Notably Trotter and Gleser~\cite{regressionBig}, used regression to estimate the relationship between different bone lengths and stature, providing study based on large dataset, incorporating aging factor and yielding groundbreaking results~\cite{statureComparisons}.

Regression is also used in cases when it is impossible to recover all parts of a human skeleton~\cite{regressionBulk} to perform traditional forensic methods. For example, hand length~\cite{regressionHandFoot, regressionHand, handNew, handNew2} or foot~\cite{regressionHandFoot,regressionFoot} can be used. Additionally, regression can predict whole-body fat mass, lean mass and trunk fat mass~\cite{regressionBodyFat} using BMI, WC, gender and age. 

\subsection{Images}

More recently image data have been used to estimate stature. Images can be calibrated or uncalibrated~\cite{estimationImages, image}. While the scale of calibrated image can be calculated with some knowledge about objects in the image, the uncalibrated image is a more complex process, often requiring reference length. A method to obtain required reference sizes is  proposed by BenAbdelkader and Yacoob~\cite{estimationImages}, by using statistical relationship in sizes of body parts. Although the methods, often do not focus primarily on anthropometric measurements, they usually rely on using stature for different tasks. 

A task related closely to task of this thesis is presented by Meunier et al.~\cite{image2DMeasurements}. Mentioned approach is based on taking pictures from two calibrated cameras and finding landmarks on the subject. The circumferences cannot be measured directly and have to be estimated using mathematical models. A different approach suggested by Hung et al.~\cite{image2DMeasurements2}, uses a single camera and a calibration square to take three pictures (front, side, back), estimating the shape from multiple images instead of using mathematical models. A more recent approach uses five images and linear regression for measurements but requires manual landmark annotation~\cite{images}.  A simple and cost-effective method uses a single webcam and an A4 sheet of paper as a scale in multiple pictures of a person to estimate dimensions using the Pixel Density Method.

\subsection{Virtual Representation}
Another step in anthropometry was brought by usage of 3D data. These are usually collected by special devices that provide extra information over usual cameras (see Section \ref{3Ddata} for more information). After all necessary data is captured, a processing algorithm (such as Iterative Closest Point~\cite{pointcloudProcess}) is run to unify data from multiple images and as result generates a point cloud or a 3D mesh~\cite{digitalAnthro}. These structures are proficient in providing the necessary means to calculate the required measurements. The measuring process can be done manually~\cite{3Dextraction}, providing the user with measurements at selected points, or by automatic landmark locating. Approach suggested by Tomi et al.~\cite{kinect} uses angle-based algorithm to calculate the width and thickness of subject's body. Then, using joint information provided by capturing device's (Kinect) SDK, finds the leftmost and rightmost points of given measurement and calculate the distance between them. For circumferences, they use ellipse perimeter equation. Different approach uses a deep neural network to extract required measurements~\cite{pointcloudNN}. This approach is based upon a template on which the measuring points are defined. The network takes the template and deforms it to look like the input. The output of this network is a deformed template with measuring points, that can be measured with point-to-point distance.


\section{Domain Adaptation}
When training a machine learning model, such as a neural network, it is crucial for the performance of the model that the training and testing data are similar and follow the same distribution~\cite{domainAdaptation}. However, this condition is often not met. One reason for this is the limited amount of training data, as demonstrated in this thesis. Another reason is the slight differences between the test data and the data the network was trained on. For instance, this can occur with medical devices where output devices may have varying colour representations. Considering factors like time complexity and data availability, domain adaptation can be an optimal solution to address these discrepancies.

Domain adaptation is a type of transfer learning, used to mitigate domain shift between source (training data) and target (data used for testing) domain~\cite{domainAdaptation}.
The assumption behind domain adaptation is, that there exists a difference in domains (source and target), but not in the task. Furthermore, the domains should not be too different and should have similar probability distributions. These limitations impact the performance, up to point where network's performance may be worse than before applying domain adaptation if used incorrectly. On the other hand, there are multiple advantages to using domain adaptation. For example, the cost of collecting and labelling new data can be mitigated. In addition, the resulting network is able to generalise better on the target domain. Multiple domain adaptation approaches have been developed to address this issue~\cite{domainAdaptationGod}.

\subsection{Instance Re-weighting Methods}
\label{Instance}
  Instance re-weighting aims to mitigate the issue of sample selection bias by assigning different weights to source domain instances based on their relevance to the target domain distribution. Sample selection bias occurs when the samples selected for model training are not representative of the population or the conditions that the model will encounter.  This bias can lead to skewed or incorrect conclusions. One of the notable approaches, Kernel Mean Matching~\cite{instanceMethod} (or KMM for short),  aims to adjust the distribution of the source domain by assigning weights to source instances. 
 
 \subsection{Feature Adaptation Methods}
 \label{Feature}
 Feature-based methods focus on finding a common feature representation that is invariant across domains. These methods typically involve transforming the feature space such that the source and target domains become indistinguishable. One important method is the Maximum Mean Discrepancy (MMD). DAN~\cite{featureMethod} incorporates MMD into a deep neural network, aligning the distributions of source and target features at multiple layers of the network. %Explain MMD
 
 Another influential method is Domain-Adversarial Neural Networks (DANN)~\cite{featureMethod2}. DANN employs adversarial training, where a domain classifier is trained to be able to distinguish source and target features while the feature extractor simultaneously learns to confuse the domain classifier. This adversarial process influences the feature extractor to generate domain-invariant features, enhancing model performance on the target domain.
 
\subsection{Classifier Adaptation Methods}
\label{Classifier}
Classifier adaptation methods aim to develop a general classifier by utilizing labeled samples from the source domain along with a few labeled samples from the target domain.

ASVM~\cite{asvm} is an example of kernel classifier-based methods. It adjusts the decision boundary learned from the source domain to fit the target domain by adding a bias function $\Delta f(x)$ to the source classifier. This bias function is parameterized by $w$ and optimized using SVM techniques to minimize classification errors on the target domain data. 
 
 \subsection{Deep Network Adaptation Methods}
Deep network adaptation is a prominent technique in domain adaptation, leveraging the powerful feature representation and end-to-end training capabilities of deep neural networks. The methods mentioned so far can be used in deep neural networks


\subsection{Adversarial Adaptation Methods}
This approach focuses on minimizing domain discrepancies by training models to generate domain-invariant features or pixel-level target samples. CyCADA (Cycle-Consistent Adversarial Domain Adaptation)~\cite{hybridMethod} combines adversarial training, cycle-consistency, and feature alignment. CyCADA uses generative adversarial networks (GANs) to generate images from domains. This approach helps align the feature distributions while preserving the semantic content, leading to improved adaptation performance.

\label{cyclegan}
CyCADA uses principles proposed by CycleGAN~\cite{CycleGAN}. Firstly, it employs a generators and discriminators to create cycle consistency. The generators are used to map images from the source domain to the target domain and then back to the original domain. The difference between input image and the image transformed back to source domain is used as a loss called cycle-consistency loss. The whole loss is then created by summing the cycle consistency loss and two generator losses.


\section{Data Acquisition}
Neural networks have caused a shift in task solving in numerous fields. The tasks, which can be neural networks used for are rather complex and require a large amount of carefully prepared datasets. To create such dataset, the data has to be collected and further correctly labelled. There are, however, many obstacles that have to be overcome to create usable dataset, of which some are:

\paragraph{Cost} Creating datasets can be expensive as they have to be large, often requiring multiple workers to collect and label data. In some cases special devices and environment may be required to be able to obtain such data, which further increases the cost.
\paragraph{Time} As the type of data vary from task to task, the data may be complicated to obtain, such as taking photographs in different regions or measuring values using complex machinery or many others. The data collection is, however, only half of the task. The other half consists of correctly labelling the data. Depending upon the task, it can sometimes require professionals to assess the data.
\paragraph{Privacy} In many medical applications the task requires personal data to make correct predictions. This raises concerns about private information leaking and thus the data has to be dealt with in way it follows all privacy regulations. Moreover, there must be enough willing patients to be able to procure such dataset, which can raise the monetary cost of such dataset. Medicine is not the only field in which personal information may be required to create suitable dataset.
\paragraph{Robustness} In order for the network to be able to correctly generalise, the dataset should be as general as is possible, including extreme and unusual cases as well. If this is not the case, the network may struggle with such cases or can develop a bias to the most common example. This is often the most complicated task, and it is rarely able to fully contain the whole spectrum.
\paragraph{Availability} Not all data is available all the time and thus obtaining can be impossible at that specific time. Most commonly this issue rises when the subject of the dataset is rare (for example certain illnesses or animals etc.) or is currently not available (wrong season or not appropriate conditions).

\subsection{Synthetic Data}
To overcome these obstacles, some data can be created synthetically~\cite{synthDataReview}. This type of data is artificially generated instead of being collected manually. The generation is based on models and algorithms designed to create samples that meet specific requirements while also containing features and variations that real data may lack. The primary goal of synthetic data is to make the network more robust, enabling it to be precise even in scenarios not covered by the real data \cite{synthDataRobust}. Besides improved precision, generating data can be significantly less expensive than collecting it. Data can be generated from existing real data, or by using domain-specific models, or by combining both methods~\cite{synthData}.

There are currently multiple approaches that are used for generating synthetic data. The simpler methods involve basic image manipulations~\cite{synthEasy}. Geometric transformations, such as rotation, scaling, cropping and flipping have proven to be useful when applied correctly. An example of incorrect use would be flipping images of CIFAR10, which can lead to label mismatch when used with numbers 6 and 9. Another group of transformations, known as photometric methods, changes the RGB channels. Examples include color jittering (replacing colours with different random or set colours), edge enhancement, and fancy PCA~\cite{fancypca}. Photometric augmentation have, however shown smaller improvement compared to geometric transformations~\cite{syntheticBasic}.

%TODO IMAGE FROM Improving Deep Learning with Generic Data Augmentation ABOUT TRANSFORMATIONS

A somewhat counterintuitive approach was proposed by Inoue~\cite{combineImages} and later improved by Summers and Dinneen~\cite{combineImprove}, who suggested mixing parts of images. This augmentation surprisingly yielded better results than the base model. Another interesting method involves removal of random segments of an image~\cite{erasing} and replacing the areas with either gray or random values, with the latter one bringing better improvement. This method is also valuable in dealing with overfitting. Both of these methods must  be used carefully to ensure they preserve the labels of the dataset.

With rise of deep learning, new methods of generating synthetic data have emerged. One of these is the Synthetic Minority Over-Sampling Technique (SMOTE~\cite{smote}), which is a widely used technique to address class imbalance issues. This method combines $k$ nearest neighbours to create new instances. Thanks to its popularity, many alternations of this method have been developed~\cite{smote2,smote3,smote4}. 

Another popular method involves using generative adversarial networks (GANs). These networks consist of two modules - discriminator and a generator. The job of the generator is to create images that resemble those in the dataset. On the other hand the job of the discriminator is to correctly determinate, whether an image is from the dataset or generated. When the discriminator assumes correctly, the generator improves, generating harder-to-distinguish images. In case the discriminator is wrong, it imporoves its ability to discern the images. This iterative learning process is then iterated over many iterations, resulting in generator capable of generating images almost indiscernible from the original dataset. One of the most popular implementations is the CycleGAN~\cite{CycleGAN} (see Section \ref{cyclegan}). A different method was proposed by Karras et al.~\cite{progressivegan}, which begins its training low-resolution images and gradually increases the resolution by adding layers. This approach's advantage is that it first discovers the large-scale structure of the images and then focuses on finer details as training progresses. To prevent shocks from new layers being added, the method proposes smoothly fading the new layers in.
\section{BMnet}
BMnet~\cite{BodyM} is a network based on MnasNet~\cite{mnasnet}, which is a convolutional neural network.  As an input it requires a frontal silhouette and optionally works with lateral silhouette as well. The lateral silhouettes provide additional information for chest and waist measurements. Moreover, the paper uses height and weight as an input metadata. The height is necessary in situations where the subjects have variable distance to camera making the scale of the subjects non-uniform. Weight then provides yet another information of the subject's size and shape. In the network the images are concatenated spatially, whilst the height and weight are concatenated depth-wise. This object is then evaluated by the MnasNet followed by MLP, consisting of 128 neurons and 14 outputs, providing the results.
The adversarial body simulation (ABS) presented in BMnet work with SMPL~\cite{smpl} model (see Section \ref{smpl}) . Its main goal is to find body shapes that are challenging for BMnet, since they are not as common or even non-existent in the dataset.
 

\section{Neural Anthropometer}
An important article is by Tejeda and Mayer~\cite{source} which proposes a method to tackle the task of estimating anthropometric measurements. The Neural Anthropometer provides a valuable approach. To keep the network as small as possible due to resource consumption and training difficulty increase with size. The proposed architecture starts with a binary image silhouette input. This is then processed by a convolutional layer. Number of channels was based on number of values on output. 

This approach is further implemented by~\cite{super}. The article further delves into different data representations of human bodies and their performance.  
